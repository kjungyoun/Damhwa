# ğŸ“ Build & ë°°í¬ ë¬¸ì„œ

## ğŸ“‘ ë²„ì „ ì •ë³´ ë° ì„¤ì • ì •ë³´

- JVM ë²„ì „ :  1.8
- ì›¹ ì„œë²„ : AWS ë°°í¬
- WAS : Apache Tomcat
- Intellij : 2020.2
- PyCharm : 2021.2
- Kotlin : 1.5.21
- Android : min SDK 24
- Swift : 5

## ğŸ“– Build ë° ë°°í¬ ì½”ë“œ

### âœï¸ Spring

- Spring í”„ë¡œì íŠ¸ ì•ˆì— Dockerfile íŒŒì¼ì„ ë§Œë“¤ê³  ì•„ë˜ ì½”ë“œë¥¼ ì…ë ¥í•œë‹¤.

```java
FROM openjdk:8-jdk-alpine
ARG JAR_FILE=build/libs/*.jar
COPY ${JAR_FILE} app.jar
ENTRYPOINT ["java","-jar","/app.jar"]
```

- gradlew íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì…ë ¥í•œë‹¤.

```bash
$ sudo ./gradlew clean

$ sudo ./gradlew build
```

- ë‹¤ìŒ build ë””ë ‰í„°ë¦¬ê°€ ìˆëŠ” ìœ„ì¹˜(spring í”„ë¡œì íŠ¸ì˜ ìµœìƒë‹¨)ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•œë‹¤.

```bash
$ docker build -t [ì´ë¯¸ì§€ì´ë¦„] .
```

- ë‹¤ìŒ ì½”ë“œë¥¼ ê³„ì†í•´ì„œ ìˆœì„œëŒ€ë¡œ ì…ë ¥í•œë‹¤.
- ì—¬ê¸°ì„œ ë„ì»¤í—ˆë¸Œ ì•„ì´ë””ë€ ì´ë©”ì¼ì´ ì•„ë‹Œ Dockerì˜ ì•„ì´ë””ë¥¼ ì˜ë¯¸í•œë‹¤.

```bash
$ docker tag [ì´ë¯¸ì§€ ì´ë¦„]:latest [ë„ì»¤í—ˆë¸Œì•„ì´ë””]/[ì´ë¯¸ì§€ì´ë¦„]:latest

$ docker push [ë„ì»¤í—ˆë¸Œì•„ì´ë””]/[ì´ë¯¸ì§€ì´ë¦„]:latest
```

- ìœ„ëŠ” ë¡œì»¬ì—ì„œ ì§„í–‰í•œ ê²ƒì´ê³  ì´ì œ EC2 ì„œë²„ì—ì„œ ì§„í–‰í•œë‹¤.
- ë¬´ì¤‘ë‹¨ ì‹¤í–‰ì‹œ í¬íŠ¸ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ë„ì»¤ í¬íŠ¸ì™€ ì´ë¯¸ì§€ í¬íŠ¸ 2ê°œë¥¼ ì„¤ì •í•´ì•¼ì§€ ì •ìƒ ì‹¤í–‰ëœë‹¤!

```bash
# EC2 ì„œë²„ì—ì„œ
$ sudo docker pull [ë„ì»¤í—ˆë¸Œì•„ì´ë””]/[ì´ë¯¸ì§€ì´ë¦„]:latest

# ë¬´ì¤‘ë‹¨ ì‹¤í–‰
$ docker run -d -p [ì‚¬ìš©í•  í¬íŠ¸] [ë„ì»¤ì´ë¯¸ì§€Name]

# ì˜ˆì‹œ (ì•í¬íŠ¸ = ë„ì»¤ í¬íŠ¸ / ë’·í¬íŠ¸ = ì´ë¯¸ì§€ í¬íŠ¸)
$ sudo docker run -d -p 8080:8080 youn0729/damhwa:latest
```

### âœï¸ Django Version

```
Django í”„ë¡œì íŠ¸ë¥¼ Docker Imageë¡œ ë¹Œë“œí•˜ì—¬ EC2 ì¸ìŠ¤í„´ìŠ¤ì— ë°°í¬í•˜ëŠ” ê²ƒ
Docker ì„¤ì¹˜, Docker Hub ê³„ì • ìƒì„±, Django í”„ë¡œì íŠ¸ ìƒì„±ì´ ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
```

- ë§ˆì´ê·¸ë ˆì´ì…˜ ì§„í–‰í•œë‹¤.

```bash
python manage.py makemigrations

pythom manage.py migrate
```

- requirements.txt ë¥¼ ìƒì„±í•œë‹¤.
- ì¦‰ pipë¡œ install ëœ í™˜ê²½ì„ txt íŒŒì¼ì— ì €ì¥í•˜ì—¬ Dockerì—ì„œë„ ë™ì¼í•˜ê²Œ êµ¬ì„±í•˜ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.

```bash
pip freeze > requirements.txt
```

### âœï¸ Dockerfile ì‘ì„±

- Django í”„ë¡œì íŠ¸ ìµœìƒë‹¨ì— Dockerfile ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ íŒŒì¼ì„ ì‘ì„±í•œë‹¤. (ë°˜ë“œì‹œ íŒŒì¼ëª…ì´ í‹€ë¦¬ë©´ ì•ˆë¨!)
- Django í”„ë¡œì íŠ¸ ìµœìƒë‹¨ì´ë¼ëŠ” ê²ƒì€ ë‹¤ì‹œ ë§í•´ì„œ manage.pyê°€ ìˆëŠ” ê²½ë¡œë¥¼ ë§í•˜ëŠ” ê²ƒì´ë‹¤.

```python
FROM python:3 # ìƒì„±í•˜ëŠ” Dockerì˜ python ë²„ì „
ENV PYTHONUNBUFFERED 1
RUN apt-get -y update
RUN apt-get -y install vim # Docker ì•ˆì—ì„œ vi ì„¤ì¹˜ ì•ˆí•´ë„ ë¨

RUN mkdir /srv/docker-server # Dockerì•ˆì— srv/docker-server í´ë” ìƒì„±
ADD . /srv/docker-server # í˜„ì¬ ë””ë ‰í„°ë¦¬ë¥¼ srv/docker-server í´ë”ì— ë³µì‚¬

WORKDIR /srv/docker-server # ì‘ì—… ë””ë ‰í„°ë¦¬ ì„¤ì •

RUN pip install --upgrade pip # pip ì—…ê·¸ë ˆì´ë“œ
RUN apt-get install build-essential
RUN pip install --upgrade pip setuptools wheel # Transformers ì„¤ì¹˜ ì—ëŸ¬ ë°©ì§€
RUN pip install -r requirements.txt --no-cache-dir # Python torch ì„¤ì¹˜ ì—ëŸ¬ ë°©ì§€

EXPOSE 8000 # 8000 port ê°œë°©
CMD ["python","manage.py","runserver","0.0.0.0:8000"] # ì‹¤í–‰
```

### âœï¸ Docker image ìƒì„±

- í˜„ì¬ ê²½ë¡œì—ì„œ Docker image buildë¥¼ ì§„í–‰í•œë‹¤.

```bash
docker build -t [ìƒì„±í•˜ê³ ì í•˜ëŠ” ë„ì»¤ ì´ë¯¸ì§€ì´ë¦„] .
```

- ì´ë¯¸ì§€ê°€ ì •ìƒì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ docker imagesë¥¼ ì…ë ¥

```bash
docker images
```

- ì´ì œ ì œëŒ€ë¡œ ëŒì•„ê°€ëŠ”ì§€ í•œë²ˆ ëŒë ¤ë³´ì

```bash
docker run -p [í˜¸ìŠ¤íŠ¸í¬íŠ¸]:[ì»¨í…Œì´ë„ˆí¬íŠ¸] [ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]

# ì˜ˆì‹œ
docker run -p 8000:8000 damhwa_django
```

- ë§Œì•½ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ê³  ì‹¶ì„ ë•ŒëŠ” ì•„ë˜ì²˜ëŸ¼ í•œë‹¤.

```bash
docker run -d -p [í˜¸ìŠ¤íŠ¸í¬íŠ¸]:[ì»¨í…Œì´ë„ˆí¬íŠ¸] [ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]

# ì˜ˆì‹œ
docker run -d -p 8000:8000 damhwa_django
```

- ë§Œì•½ ìœ„ì—ì„œ ì œëŒ€ë¡œ ì‹¤í–‰ ë˜ì—ˆë‹¤ë©´ ì´ì œ Docker í—ˆë¸Œì— push í•´ì„œ EC2 ì„œë²„ì—ì„œ ë¶ˆëŸ¬ì™€ë³´ì

### ğŸ“Œ Docker Hub ì— Image íŒŒì¼ Push í•˜ê¸°

- Docker Hub ê³„ì •ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•œë‹¤.
- ì•„ë˜ ëª…ë ¹ì–´ë¡œ push í•´ë³´ì

```bash
docker tag [ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]:latest [ë„ì»¤í—ˆë¸Œì•„ì´ë””]/[ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]:latest

docker push [ë„ì»¤í—ˆë¸Œì•„ì´ë””]/[ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]:latest
```

- ì´ì œ Docker í—ˆë¸Œë¥¼ í™•ì¸í•´ë³´ë©´ í•´ë‹¹ ì´ë¯¸ì§€ê°€ ì˜¬ë¼ê°€ìˆì„ ê²ƒì´ë‹¤.

### ğŸ“Œ EC2 ì„œë²„ì—ì„œ pull ë°›ì•„ ì‹¤í–‰í•˜ê¸°

- ì´ì œ Docker Hubì—ì„œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰ì‹œí‚¤ë©´ ë!
- ì‚¬ìš©í•  ì»¨í…Œì´ë„ˆì˜ ë³„ì¹­ì„ ì§€ì •í•´ì£¼ë©´ í•´ë‹¹ ì»¨í…Œì´ë„ˆì˜ IDë¥¼ ë‚´ê°€ ì§€ì •í•œ ë³„ì¹­ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```bash
docker pull [ë„ì»¤í—ˆë¸Œì•„ì´ë””]/[ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]:latest

docker run -d -p [í˜¸ìŠ¤íŠ¸í¬íŠ¸]:[ì»¨í…Œì´ë„ˆí¬íŠ¸] [ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]

docker run -d -p 8000:8000 --name [ì‚¬ìš©í•  ì»¨í…Œì´ë„ˆë³„ì¹­][ìƒì„±í•œ ì´ë¯¸ì§€ ì´ë¦„]

# ì˜ˆì‹œ
docker run -d -p 8000:8000 damhwa_django
```

## âš ï¸ ë°°í¬ ì‹œ íŠ¹ì´ì‚¬í•­

- Django ì„œë²„ì™€ Spring ì„œë²„ ë‘˜ ë‹¤ ë°°í¬
- Django ì„œë²„ í¬íŠ¸ : 8000
- Spring ì„œë²„ í¬íŠ¸ : 8080
- Docker Hubë¡œ ë°°í¬ ì„±ê³µ

## ğŸ“œ DB ì •ë³´

- ë°°í¬ìš© DB ì •ë³´

  -> Hostname(ì ‘ì† ì£¼ì†Œ) : 3.38.83.30 / ë„ë©”ì¸ : j5a503.p.ssafy.io

  -> Username : ssafy

  -> UserPassword : ssafy
  
  ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-10-05 á„‹á…©á„’á…® 10 18 49](https://user-images.githubusercontent.com/48318620/136031000-7f4530d3-8496-4a4c-b4c7-9dc3f35c05b0.png)


- ER-Diagram

    <img width="901" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-10-05 á„‹á…©á„’á…® 10 23 16" src="https://user-images.githubusercontent.com/48318620/136031526-93cf4322-3d21-4dd1-b8bf-a506ea143bec.png">

    
## ğŸ“Œ ì¸ê³µì§€ëŠ¥ Model

- SKT Brain KoBERT Model : í•œêµ­ì–´ ë²„ì „ì˜ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸
- ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì§„í–‰í•œ í•™ìŠµì€ ê¸°ì¡´ì— í•™ìŠµëœ KoBERT ëª¨ë¸ì— ê°ì • ì–¸ì–´ë¥¼ ì£¼ì…í–ˆë˜ Fine Tuningì´ë‹¤.
- AIhubì— ì˜¬ë¼ì™€ ìˆëŠ” ì•½ 7ë§Œì—¬ê°œì˜ í•œêµ­ì–´ ë§ë­‰ì¹˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì´ 6ê°œì˜ ê°ì • ëŒ€ë¶„ë¥˜ ëª¨í˜•ì„ ë§Œë“¤ì—ˆë‹¤.
- testë°ì´í„°ë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•œ ê²°ê³¼ ì•½ 86%ì˜ ì •í™•ë„ë¥¼ ì§€ë‹ˆê³  ìˆë‹¤.
- KoBERTë¥¼ ì´ì•¼ê¸° í•˜ê¸°ì „ì— BERTì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ë” ì„¤ëª…í•˜ìë©´ ì•„ë˜ì™€ ê°™ë‹¤.

#### ğŸ¤– BERTì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì„¤ëª…

- ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ 2019ë…„ì— í•œ í•™ìˆ ì§€ì— ì—„ì²­ë‚œ ì„±ëŠ¥ì„ ì§€ë‹Œ ëª¨ë¸ì´ ë‚˜ì™”ìœ¼ë©°. ê·¸ê²Œ ë°”ë¡œ BERTì´ë‹¤.
- BERTëŠ” Bidirectional Encoder Representations from Transformerì˜ ì•½ìë¡œ í…ìŠ¤íŠ¸ë¥¼ ì–‘ë°©í–¥(ì•ë’¤)ë¡œ í™•ì¸í•˜ì—¬ ìì—°ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ê¸°ì¡´ì˜ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì€ ë‹¨ë°©í–¥ ìš°ë¦¬ê°€ ê¸€ì„ ì½ëŠ” ìˆœì„œì¸ ì™¼ìª½â†’ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°”ì§€ë§Œ BERTëŠ” ì´ ìˆœì„œë¥¼ ì–‘ë°©í–¥ìœ¼ë¡œ ë³´ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•´ ë§¤ìš° ë†’ì€ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê·¸ë¦¬ê³  ë¬´ì—‡ë³´ë‹¤ ëŒ€í˜• ì¸ê³µì§€ëŠ¥ì˜ ì¼ì¢…(êµ¬ê¸€ì—ì„œ ê°œë°œ)ì´ê³  ì˜¤í”ˆì†ŒìŠ¤ì´ê¸° ë•Œë¬¸ì— ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆë‹¤.
- ë³¸ íŠ¹í™” í”„ë¡œì íŠ¸ì—ì„œëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸ êµ¬ì¶•ì—ì„œ Colabì„ í™œìš©í–ˆê³  GPUë¥¼ í†µí•´ ëª¨ë¸ í•™ìŠµì— ê±¸ë¦° ì‹œê°„ì€ 2ì‹œê°„ì´ë©° í•˜ë‚˜ì˜ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ”ë° ë§ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤. ë”°ë¼ì„œ Colab í™˜ê²½ì—ì„œ ì§€ì›í•˜ëŠ” GPU ë˜ëŠ” TPUë¥¼ ì‚¬ìš©í•˜ì. CPUë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë©´ ì•„ì— ì§„í–‰ë˜ì§€ ì•ŠìŒì„ ëŠë‚€ë‹¤.
- ê·¸ë¦¬ê³  KoBERTë¼ëŠ” ê²ƒì€ í•œêµ­ì–´ì˜ ê²½ìš° ë‹¤ë¥¸ ë‚˜ë¼ì˜ ì–¸ì–´ë³´ë‹¤ í›¨ì”¬ ë³µì¡í•´ì„œ SKT-brainì—ì„œ BERTì˜ í•œêµ­ì–´ ë²„ì „ì„ ë§Œë“¤ì—ˆë‹¤. ì˜ˆì¸¡ë¥ ì´ í›¨ì”¬ ì¢‹ë‹¤ê³ í•œë‹¤. íŠ¹íˆ ê´€ë ¨ ì´ìŠˆë¥¼ [https://github.com/SKTBrain/KoBERT/pulls?q=is%3Apr+is%3Aclosed](https://github.com/SKTBrain/KoBERT/pulls?q=is%3Apr+is%3Aclosed) ì—ì„œ ê²€ìƒ‰í•˜ì—¬ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ë©´ ëœë‹¤.
- BERTë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” `kobert, pytorch`ì´ë‹¤.

##### âœï¸ Colabí™˜ê²½ì—ì„œ KoBERT í•™ìŠµí•˜ê¸°

```bash
!pip install mxnet # ì½”ë© í™˜ê²½ì´ê¸° ë•Œë¬¸ì— ì•ì— !ë¥¼ ë¶™ì—¬ì•¼ í•œë‹¤.
!pip install gluonnlp pandas tqdm
!pip install sentencepiece
!pip install transformers==3.0.2
!pip install torch
```

KoBERTë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìœ„ì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆì–´ì•¼ í•œë‹¤. íŠ¹íˆ KoBERTì—ì„œëŠ”  mxnet, torch, gluonnlpë¥¼ í•„íˆ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•˜ë©°, BERT ëª¨ë¸ ê³µí†µì ìœ¼ë¡œ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆì–´ì•¼ í•œë‹¤. ë˜í•œ python ë²„ì „ì— ë”°ë¼ ì‘ë™í•˜ëŠ” transformersì˜ ë²„ì „ì´ ìƒì´í•˜ë¯€ë¡œ ì´ë¥¼ ì£¼ì˜í•˜ì. ë³¸ íŠ¹í™” í”„ë¡œì íŠ¸ì—ì„œëŠ” python==3.7.xì— transformers==3.0.2ë¥¼ ì‚¬ìš©í–ˆë‹¤. ê·¸ë˜ì„œ ìœ„ì™€ ê°™ì´ ë‹¤ìš´ë¡œë“œë¥¼ ë°›ì•„ì¤€ë‹¤.

ë˜í•œ ë§Œì•½ì— ë‚´ê°€ KoBERTë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ë©´

```bash
!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master
```

ìœ„ì˜ ì½”ë“œë¡œ KoBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ íŒ¨í‚¤ì§€ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼í•œë‹¤.

```python
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook
```

í•„ìš”í•œ í•¨ìˆ˜ë¥¼ ëª¨ë‘ loadí•´ì¤€ë‹¤. ë‚˜ë„ ìœ„ì— ìˆëŠ” ëª¨ë“  í•¨ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ì•Œì§€ëŠ” ëª»í•œë‹¤. ë‹¨ì§€ KoBERTë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ ì•ì—ì„œ ë¯¸ë¦¬ ì„¤ì •í•´ì•¼ í•  í•¨ìˆ˜ë“¤ì´ë‹¤.

```python
#kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model

#transformers
from transformers import AdamW # ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ ì´ˆê¸°ê°’ ì§€ì • í•¨ìˆ˜ë¥¼ ì•„ë‹´ìœ¼ë¡œ ì§€ì •í•œë‹¤.
from transformers.optimization import get_cosine_schedule_with_warmup
```

kobert ë¶€ë¶„ì€ ìš°ë¦¬ê°€ í•™ìŠµí•  Kobert ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì„¤ì •í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ë‹ˆ ìš°ë¦¬ê°€ ì¸ê³µì§€ëŠ¥ì„ ë§Œë‹¤ëŠ”ë°, ì™œ ì €ëŸ°ê±¸ ì„¤ì •í•´? ë¼ê³  ìƒê°í•œë‹¤ë©´, ì¼ë‹¨ BERT ëª¨í˜• ìì²´ê°€ ì˜¤í”ˆì†ŒìŠ¤ë‹¤. ì´ë¯¸ êµ¬ê¸€ì—ì„œ 104ê°œì˜ ì–¸ì–´ë¥¼ êµ¬ë¶„í•´ì„œ í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ì´ë‹¤. ê·¸ë ‡ë‹¤ ì´ë¯¸ í•™ìŠµí–ˆë‹¤. ê·¸ê±¸ ìš°ë¦¬ê°€ ê°–ì ¸ë‹¤ê°€ ì“°ëŠ”ê±°ë‹¤. ê·¸ëŸ¬ë©´ ë¬´ì—‡ì„ í•™ìŠµí–ˆëŠ”ê°€? ì•„ë§ˆ í•™ ë‚˜ë¼ì˜ ì–¸ì–´ë³„ íŠ¹ì§•ì„ í•™ìŠµí•˜ì§€ ì•Šì•˜ë‚˜ ìƒê°ë“ ë‹¤. ê³µì‹ë¬¸ì„œ ë“¤ì–´ê°€ì„œ í™•ì¸í•´ë³´ë©´ ë˜ê² ë‹¤.

ê·¸ë¦¬ê³  BERT ëª¨í˜•ì€ í™œì„±í™” í•¨ìˆ˜ë¥¼ softmaxí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ë˜ì„œ ì…ë ¥ê°’ìœ¼ë¡œ ì¸í•´ ì¶œë ¥ëœ ê°’ì€ ëª¨ë‘ 0~1ì‚¬ì´ì˜ ê°’ì´ê³  ë‹¤ ë” í–ˆì„ ë•Œ 1ì´ëœë‹¤. ê·¸ë ‡ë‹¤ ê·¸ëƒ¥ í™•ë¥ ì´ë‹¤.

(ì—„ë°€íˆ í™•ë¥ ì´ëƒ? ë¼ê³  í–ˆì„ ë•Œ ê·¸ë ‡ê²Œ ë´ë„ ë¬´ë°©í•˜ë‹¤. ì–´ì°¨í”¼ ì¸ê³µì§€ëŠ¥ë„ weightë¶€ë¶„ì„ ëª¨ìˆ˜ë¡œ ì¶”ì •í•œë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ ìµœì ì˜ weightì„ ì°¾ëŠ”ê±´ë°, ì´ë•Œ ì´ weightì„ í™•ë¥ ë³€ìˆ˜ë¡œ ë³¸ë‹¤. ê·¸ë ‡ë‹¤ ê·¸ëƒ¥ í™•ë¥ ì´ë‹¤.)

```python
#GPU ì‚¬ìš©
device = torch.device("cuda:0")
```

ì¸ê³µì§€ëŠ¥ì€ GPUì—†ìœ¼ë©´ ê·¸ëƒ¥ ë¤í”„ì™€ë„ ê°™ë‹¤. ì—°ì‚°ì´ ìƒëª… CPUë¡œ í•˜ì§€ë§ì 1ë…„ì´ ë„˜ì–´ë„ ì•ˆëë‚  ìˆ˜ ìˆë‹¤. ê·¸ë‹ˆê¹Œ GPU ì‚¬ìš©ì„ ì„¤ì •í•´ì£¼ì. ì£¼ë³€ ì´ì•¼ê¸°ë¡œëŠ” TPUë¥¼ ì“°ë¼ê³  í•˜ëŠ”ë°, TPUë¥¼ ì“°ë©´ ì½”ë“œê°€ ì¢€ ë‹¬ë¼ì§„ë‹¤. ê·¸ë˜ì„œ ì¼ë¶€ëŸ¬ GPUë¡œ ë¨¼ì € ì²´í—˜í•˜ëŠ” ì‹ìœ¼ë¡œ ì‚¬ìš©í•´ë³´ì.

ëˆˆì¹˜ ë¹ ë¥¸ ì‚¬ëŒì„ ì•Œê² ì§€ë§Œ, GPUëŠ” Colab ì•ˆì— ìˆë‹¤.

```python
import os

n_devices = torch.cuda.device_count()
print(n_devices)

for i in range(n_devices):
    print(torch.cuda.get_device_name(i))
```

`cuda.device_count()`ê°€ í˜„ì¬ ì‚¬ìš©í•˜ëŠ” GPU ê°œìˆ˜ì¢€ ì•Œë ¤ë‹¬ë¼ëŠ” ê±´ë°, ì €ê²Œ 0ì´ ëœ¨ë©´ GPUì•ˆì“°ê³  ìˆëŠ”ê±°ë‹¤. ê·¸ë‹ˆê¹Œ. ê¼­ í™•ì¸í•´ë³´ê³  ì•ˆëœ¨ë©´, ì™¼ìª½ ìƒë‹¨ì— ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ í™˜ê²½ ë³€ê²½ -> GPUë¡œ ë³€ê²½í•´ì£¼ì.

```python
if torch.cuda.is_available():    
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print('No GPU available, using the CPU instead.')
```

ê·¸ë ‡ë‹¤ë©´, ë§ˆì§€ë§‰ìœ¼ë¡œ GPU ì‚¬ìš©ê°€ëŠ¥í•œì§€ ì²´í¬í•´ë³´ê³  GPUì˜ ì´ë¦„ì„ ë³¼ ìˆ˜ ìˆë„ë¡ ì„¸íŒ…í•˜ì.

```python
#BERT ëª¨ë¸, Vocabulary ë¶ˆëŸ¬ì˜¤ê¸°
bertmodel, vocab = get_pytorch_kobert_model()
```

ë“œë””ì–´ BERT ëª¨í˜•ì„ ë¶ˆëŸ¬ì™”ë‹¤. bertmodelì€ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì´ ì €ì¥, vocabëŠ” ì‚¬ìš©ë˜ëŠ” í•œêµ­ì–´ ë‹¨ì–´ê°€ ì €ì¥ ì°¾ã…‡ìë³´ë‹ˆ vocabì—ëŠ” 8000ì—¬ê°œì˜ í•œêµ­ì–´ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ ìˆë‹¤ê³  í•œë‹¤. ê·¼ë° ì´ê±° ë§¤ìš° ì ì€ê±°ë‹¤. ê·¸ë˜ì„œ KoBERTì˜ í•œê³„ì ì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤.

ê·¸ë¦¬ê³  ì´ë•Œë¶€í„° ì‚´ì§ ë¸Œë¼ìš°ì €ì˜ ë°˜ì‘ì´ ëŠ¦ê²Œ ì˜¨ë‹¤. (ì‹œê°„ì´ ê±¸ë¦°ë‹¤...ã… )

```python
import pandas as pd
naturalTraining_data = pd.read_excel('.../á„€á…¡á†·á„‰á…¥á†¼á„ƒá…¢á„’á…ªá„†á…¡á†¯á„†á…®á†¼á„á…µ(á„á…¬á„Œá…©á†¼á„ƒá…¦á„‹á…µá„á…¥)_Training.xlsx')
```

ê¸°ë³¸ì ìœ¼ë¡œ ë°ì´í„° í”„ë ˆì„í˜•ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤.

ì´ì œ ì € ìœ„ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ë³´ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„°ì˜ ìƒê¹€ìƒˆë¥¼ í™•ì¸í•´ë´ì•¼í•œë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¡œ ì¼ë¶€ë¶„ë§Œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
naturalTraining_data.head()
naturalTraining_data.sample(n=10)
```

ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•œ í›„ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì„¸íŒ…ì— ë“¤ì–´ê°„ë‹¤.

```python
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))  
```

KoBERT ëª¨ë¸ì— ë“¤ì–´ê°ˆ ë°ì´í„° ì…‹ì— ëŒ€í•œ classì´ë‹¤.

```python
# Setting parameters í•„ìˆ˜
max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 15
max_grad_norm = 1
log_interval = 100
learning_rate =  5e-5
```

ìœ„ì˜ parameterë¥¼ í†µí•´ ì¸ê³µì§€ëŠ¥ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

```python
#í† í°í™”
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)
data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)

train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)
test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)
```

```python
class BERTClassifier(nn.Module): ## í´ë˜ìŠ¤ë¥¼ ìƒì†
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=6,   ##í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)
```

í´ë˜ìŠ¤ ìˆ˜ 6ê°œë¥¼ ì¡°ì •í•˜ê³  ì´ë¥¼ í†µí•´ ì¸ê³µì§€ëŠ¥ì˜ black Boxì¸ hidden layerê¹Œì§€ì˜ ì„¸íŒ…ì„ ëª¨ë‘ ê°–ì¶˜ë‹¤.

```python

#BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)
#optimizerì™€ schedule ì„¤ì •
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()

t_total = len(train_dataloader) * num_epochs
warmup_step = int(t_total * warmup_ratio)

scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)

#ì •í™•ë„ ì¸¡ì •ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc
    
train_dataloader

for e in range(num_epochs):
    train_acc = 0.0
    test_acc = 0.0
    model.train()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):
        optimizer.zero_grad()
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        loss = loss_fn(out, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        scheduler.step()  # Update learning rate schedule
        train_acc += calc_accuracy(out, label)
        if batch_id % log_interval == 0:
            print("epoch {} batch id {} loss {} train acc {}".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))
    print("epoch {} train acc {}".format(e+1, train_acc / (batch_id+1)))
    
    model.eval()
    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)
        valid_length= valid_length
        label = label.long().to(device)
        out = model(token_ids, valid_length, segment_ids)
        test_acc += calc_accuracy(out, label)
    print("epoch {} test acc {}".format(e+1, test_acc / (batch_id+1)))
```

í•™ìŠµ ì§„í–‰

```python
## í•™ìŠµ ëª¨ë¸ ì €ì¥
PATH = 'drive/MyDrive/colab/StoryFlower/bert' # google ë“œë¼ì´ë¸Œ ì—°ë™ í•´ì•¼í•¨. ê´€ë ¨ì½”ë“œëŠ” ëºìŒ
torch.save(model, PATH + 'KoBERT_ë‹´í™”.pt')  # ì „ì²´ ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), PATH + 'model_state_dict.pt')  # ëª¨ë¸ ê°ì²´ì˜ state_dict ì €ì¥
torch.save({
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict()
}, PATH + 'all.tar')  # ì—¬ëŸ¬ ê°€ì§€ ê°’ ì €ì¥, í•™ìŠµ ì¤‘ ì§„í–‰ ìƒí™© ì €ì¥ì„ ìœ„í•´ epoch, loss ê°’ ë“± ì¼ë°˜ scalarê°’ ì €ì¥ ê°€ëŠ¥
```

ëª¨ë¸ì„ ì €ì¥í•œ ì´í›„ í•™ìŠµí•œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•´ì•¼í•  ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
!pip install mxnet
!pip install gluonnlp pandas tqdm
!pip install sentencepiece
!pip install transformers==3.0.2
!pip install torch

!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master

# torch
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

#kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model

#GPU ì‚¬ìš©
device = torch.device("cuda:0")

#BERT ëª¨ë¸, Vocabulary ë¶ˆëŸ¬ì˜¤ê¸° í•„ìˆ˜
bertmodel, vocab = get_pytorch_kobert_model()


# KoBERTì— ì…ë ¥ë  ë°ì´í„°ì…‹ ì •ë¦¬
class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,
                 pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))  

# ëª¨ë¸ ì •ì˜
class BERTClassifier(nn.Module): ## í´ë˜ìŠ¤ë¥¼ ìƒì†
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=6,   ##í´ë˜ìŠ¤ ìˆ˜ ì¡°ì •##
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):
        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)
        
        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))
        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

# Setting parameters
max_len = 64
batch_size = 32
warmup_ratio = 0.1
num_epochs = 20
max_grad_norm = 1
log_interval = 100
learning_rate =  5e-5

## í•™ìŠµ ëª¨ë¸ ë¡œë“œ
PATH = 'drive/MyDrive/colab/StoryFlower/bert/'
model = torch.load(PATH + 'KoBERT_ë‹´í™”_86.pt')  # ì „ì²´ ëª¨ë¸ì„ í†µì§¸ë¡œ ë¶ˆëŸ¬ì˜´, í´ë˜ìŠ¤ ì„ ì–¸ í•„ìˆ˜
model.load_state_dict(torch.load(PATH + 'model_state_dict_86.pt'))  # state_dictë¥¼ ë¶ˆëŸ¬ ì˜¨ í›„, ëª¨ë¸ì— ì €ì¥

#í† í°í™”
tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

def new_softmax(a) : 
    c = np.max(a) # ìµœëŒ“ê°’
    exp_a = np.exp(a-c) # ê°ê°ì˜ ì›ì†Œì— ìµœëŒ“ê°’ì„ ëº€ ê°’ì— expë¥¼ ì·¨í•œë‹¤. (ì´ë¥¼ í†µí•´ overflow ë°©ì§€)
    sum_exp_a = np.sum(exp_a)
    y = (exp_a / sum_exp_a) * 100
    return np.round(y, 3)


# ì˜ˆì¸¡ ëª¨ë¸ ì„¤ì •
def predict(predict_sentence):

    data = [predict_sentence, '0']
    dataset_another = [data]

    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)
    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)
    
    model.eval()

    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):
        token_ids = token_ids.long().to(device)
        segment_ids = segment_ids.long().to(device)

        valid_length= valid_length
        label = label.long().to(device)

        out = model(token_ids, valid_length, segment_ids)

        test_eval=[]
        for i in out:
            logits=i
            logits = logits.detach().cpu().numpy()
            min_v = min(logits)
            total = 0
            probability = []
            logits = np.round(new_softmax(logits), 3).tolist()
            for logit in logits:
                print(logit)
                probability.append(np.round(logit, 3))

            if np.argmax(logits) == 0:  emotion = "ê¸°ì¨"
            elif np.argmax(logits) == 1: emotion = "ë¶ˆì•ˆ"
            elif np.argmax(logits) == 2: emotion = 'ë‹¹í™©'
            elif np.argmax(logits) == 3: emotion = 'ìŠ¬í””'
            elif np.argmax(logits) == 4: emotion = 'ë¶„ë…¸'
            elif np.argmax(logits) == 5: emotion = 'ìƒì²˜'

            probability.append(emotion)
            print(probability)
    return probability
```

ì´ì œ ìœ„ ì½”ë“œë¥¼ django ì„œë²„ì— ë°˜ì˜í•˜ë©´ ëœë‹¤.

íŠ¹íˆ í•™ìŠµí•œ ëª¨ë¸ì„ djangoì— ë¡œë“œí•  ë•Œ í•„ìš”í•œ í´ë˜ìŠ¤ì¸ `BERTDataset`ì™€ `BERTClassifier`ì€ manage.pyì— ì„¸íŒ…í•œ ë’¤ì— ëª¨ë¸ì„ Apps.pyì—ì„œ ë¶ˆëŸ¬ì˜¤ì.

## ğŸ“Œ KNN ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜

- ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê³µì‹ì„ ì´ìš©í•œ ìœ ì‚¬ ê°ì • ê½ƒ ì¶”ì²œ

- ë§¤ìš° ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ì´ë©°, ì½”ë“œ ì—°ì‚°ì— í° ì–´ë ¤ì›€ì´ ì—†ê¸° ë•Œë¬¸ì— ìì„¸í•œ ë¶€ë¶„ì€ ìƒëµí•œë‹¤.

  **Why KNN ?**

  - KoBERT ëª¨ë¸ì— í…ìŠ¤íŠ¸ë¥¼ ëŒ€ì… í›„ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¬¼ì€ í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ì•„ë‹Œ íŠ¹ì •ì— ê°ì •ì— ì†í•  ê°€ì¤‘ì¹˜ ê°’ì´ë‹¤. ê·¸ ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ê°ì •ì¼ í™•ë¥ ì´ ë†’ìŒì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì£¼ì˜í•´ì•¼í•  ë¶€ë¶„ì´ê¸°ë„ í•˜ë‹¤. í•œ ì‚¬ëŒì´ ì…ë ¥í•œ í…ìŠ¤íŠ¸ ì•ˆì—ëŠ” í•˜ë‚˜ì˜ ê°ì •ë§Œ ë‹´ê²¨ìˆì§€ ì•Šë‹¤. ë”°ë¼ì„œ 6ê°œì˜ ê°ì • ëŒ€ë¶„ë¥˜ë¥¼ ëª¨ë‘ ê²€ì‚¬í•˜ì—¬ ê½ƒë§ + ê½ƒì˜ ë°°ê²½ìœ¼ë¡œ ì¸í•´ ë“±ì¥í•œ í™•ë¥ ê°’ê³¼ ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ë¡œ ì¸í•´ ë“±ì¥í•œ í™•ë¥ ê°’ ì‚¬ì´ì˜ ê°€ì¥ ìµœì†Œì˜ ê±°ë¦¬ë¥¼ ì§€ë‹Œ ê½ƒì„ ì¶”ì²œí•˜ëŠ” ê²ƒì´ ëª¨ë“  ê°ì •ì„ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê½ƒì„ ì¶”ì²œí•˜ëŠ” ë§¥ë½ì— ì–´ìš¸ë¦¬ê¸° ë•Œë¬¸ì— ì‹¤ìˆ˜ë¥¼ ì¹˜ì—­ìœ¼ë¡œ í•œ ê°€ì¤‘ì¹˜ ê°’ì„ softmaxí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í–ˆê³  ì´ë¥¼ í†µí•´ ê½ƒì„ ì¶”ì²œí•˜ëŠ” KNN ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆë‹¤.

``` python
# rest_framework
from rest_framework import status
from rest_framework.decorators import api_view
from rest_framework.response import Response
from django.db import connection
import numpy as np
import pandas as pd
import sys
from os import path
# Message Recommend
@api_view(['POST', 'GET'])
def msg_recomm(request):
    if request.method == 'POST':
        print("Django Success!")
        data = request.data.get('msg') # Spring ìš”ì²­ ë°ì´í„°
        print("request data : " + data)

        # KoBert ê°ì • ë¶„ì„ ëª¨ë¸
        # model_result = [21.45123, 10.1234, 4.012312, 4.01234, 31.43234, 13.123415]
        sys.path.append(path.join(path.dirname(__file__), '..'))
        from kobert_predict import predict
        model_result = predict(data)

        # knn ì•Œê³ ë¦¬ì¦˜
        flag = True
        datas = knn(model_result, flag)
        print (datas)

        return Response(data=datas, status=status.HTTP_200_OK)

# State Recommend
@api_view(['POST'])
def state_recomm(request):
    if request.method == 'POST':
        print("Django Success!")
        data = request.data.get('state') # Request data
        print("request data : " + data)

        # KoBert ê°ì • ë¶„ì„ ëª¨ë¸ load
        sys.path.append(path.join(path.dirname(__file__), '..'))
        from kobert_predict import predict
        model_result = predict(data)
        state = model_result[6]

        # knn ì•Œê³ ë¦¬ì¦˜
        flag = False
        datas = knn(model_result, flag)
        response = {
            'fno': datas,
            'state' : state
        }
        print(datas)

        return Response(data=response, status=status.HTTP_200_OK)

def knn(model_result, flag):
    # DB emotion ì¡°íšŒ
    try:
        cursor = connection.cursor()
        strSql = "SELECT fno, happy, unstable, embarrass, sad, angry, hurt FROM emotion"
        result = cursor.execute(strSql)
        emotion = cursor.fetchall()

        connection.commit()
        connection.close()

        datas = []
        for data in emotion:
            # DB í™•ë¥ ê°’ë§Œ ì €ì¥
            tmp = [data[1], data[2], data[3], data[4], data[5], data[6]]

            # ìœ í´ë¦¬ë””ì•ˆ distance
            sum = 0
            for i in range(0, len(tmp)):
                df = model_result[i] - tmp[i]  # ë°°ì—´ê°„ ëº„ì…ˆ
                df = df ** 2  # ë°ì´í„°ì˜ ì œê³±
                sum += df

            row = {
                'fno': data[0],  # flower primary key
                'distance': np.sqrt(sum)  # ë°ì´í„°ë“¤ì˜ í•©ì˜ ì œê³±ê·¼ = ê±°ë¦¬
            }

            datas.append(row)

        df1 = pd.DataFrame(datas,columns=['fno','distance']) # ê²°ê³¼ dataframe ìƒì„±
        df1 = df1.sort_values('distance').head(5) # distanceê°€ ê°€ì¥ ì‘ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 5ê°œ ì¶”ì¶œ
        print(df1)

        # ìƒìœ„ 5ê°œ fno listë¡œ ì¶”ì¶œ
        result_fno = []
        for index, row in df1.iterrows():
            result_fno.append(int(row['fno']))

        if(flag):
            return result_fno
        else:
            return result_fno[0]
    except:
        connection.rollback()
        print("Failed selecting in emotion")
```

djangoì— ë°˜ì˜ëœ ì½”ë“œëŠ” ìœ„ì™€ ê°™ë‹¤.